---
title: "HW 3"
author: "Pranav Kallem"
date: "10/10/2024"
output: 
  html_document:
    number_sections: true
---

#

Let $E[X] = \mu$.  Show that $Var[X] := E[(X-E[X])^2] = E[X^2]-(E[X])^2$.  Note, all you have to do is show the second equality (the first is our definition from class). 
We start with the definition of variance: $$
Var[X] = E[(X - E[X])^2]
$$ Expanding the square: $$
Var[X] = E[X^2 - 2X E[X] + (E[X])^2]
$$ Breaking this into separate expected values: $$
Var[X] = E[X^2] - 2E[X]E[X] + (E[X])^2
$$ $$
Var[X] = E[X^2] - (E[X])^2
$$ Thus, $Var[X] = E[X^2] - (E[X])^2$.


# 

In the computational section of this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
set.seed(1)
train=sample(200,100)
svmfit = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
plot(svmfit, dat[train,])
```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
svmfit = svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 1, cost = 10000)
plot(svmfit, dat[train,])
```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

*The decision boundary captures the data more closely, but this comes with the risk of overfitting. A high-cost value causes the model to fit the training data too closely, potentially sacrificing generalization to new data.*

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
#remove eval = FALSE in above
table(true=dat[-train,"y"], pred=predict(svmfit, newdata=dat[-train,]))
```


##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
sum(dat[train,3] == 2)/100

```

*The discrepancy seems to stem more from overfitting the decision boundary rather than an imbalance between the training and testing sets. Even if the training data is well-representative, an irregular decision boundary may still lead to a biased model. That said, the 29 percent result is fairly close to the underlying proportion.*

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}

set.seed(1)
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1,2,3,4)))
summary(tune.out)

```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

*We don't seem to be overfitting as much anymore, since the decision boundary is likely smoother and the error rate is at its lowest, even on the testing set. However, because the original data had uneven class distribution, the classifier misclassifies class 1 as class 2 more often than the other way around. This isn't really a problem with the model, but more a result of the data, and it's important to keep that in mind.*

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}

for (i in 1:length(heart$class)) {
  if (heart$class[i] > 0){
    heart$class[i] = 1
  }
}

heart$class = as.factor(heart$class)
```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)
train=sample(1:nrow(heart), 240)

tree.heart = tree(class~., heart, subset=train)
plot(tree.heart)
text(tree.heart, pretty=0)
```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}

tree.pred = predict(tree.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, class))
1-(28+18)/57
```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
set.seed(101)
cv.heart = cv.tree(tree.heart, FUN = prune.misclass)
cv.heart

plot(cv.heart$size, cv.heart$dev, type = "b")

prune.heart = prune.misclass(tree.heart, best =3)
plot(prune.heart)
text(prune.heart, pretty=0)

tree.pred = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, class))
1-((26+17)/57)
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

*We've given up a small amount of classification accuracy (around 4%) in exchange for a much more understandable decision tree. It looks like thal is the most important variable, followed by ca and cp.*

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

*Decision trees can become biased if the training data is unbalanced or biased, resulting in unfair predictions that favor certain groups. The model tends to focus on attributes that maximize information gain, which can lead it to rely on biased features, like demographic factors, reinforcing unfair outcomes. Overfitting makes this worse by capturing noise or irrelevant patterns in the data. To prevent this, itâ€™s important to use well-prepared, balanced data and avoid biased features. Techniques like pruning or using ensemble methods can also help reduce bias and improve fairness.*